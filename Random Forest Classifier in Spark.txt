                                                               RANDOM FOREST CLASSIFIER IN SPARK



-- From the terminal start spark shell

spark-shell

-- Creating the database
sqlContext.sql("create database vz")

-- Create table using the command below to store data in hive tables.

sqlContext.sql("CREATE TABLE m_d(day string,hr int,enodeb int,eutrancell string,carrier_x string,sip_sc_abnormal_releases double,sip_sc_call_answers double,sip_sc_call_attempts double,sip_sc_call_completions double,sip_sc_call_drops double,sip_sc_call_drops_including_handover double,sip_sc_call_setup_completes double,sip_sc_call_setup_failures double,adjusted_sip_sc_dcs double,adjusted_sip_sc_dc_pct double,handover_attempts double,handoverfailures double,handover_failure_pct double,qci1_bd_pct_num double,qci1_bd_pct_den double,key_ramesh string,centerline_type int,lat_new double,long_new double,key_frequency string,dc_perc double,loc_id int,enode_b int,summary string,icon string,precipIntensity double,precipProbability double,temperature double,apparentTemperature double,dewPoint double,humidity double,pressure double,windSpeed double,windGust double,windBearing double,cloudCover double,uvIndex double,visibility double,ozone double,precipType string,action int) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TextFile");

-- Using the database created
sqlContext.sql("use vz");

-- Pushing data from csv to hive table we created.
sqlContext.sql("LOAD DATA LOCAL INPATH '/home/cloudera/Desktop/test_vz_data.csv' OVERWRITE INTO TABLE m_d")

-- To check if the data is pushed correctly.
sqlContext.sql("Select * from m_d").show()

##############################################################Dispatch decision#####################################################################
sqlContext.sql("use vz");

val merged_data= sqlContext.sql("SELECT * FROM m_d where dc_perc>0")
merged_data.count

/* Dropping NULL values from the data */
val merged_data_out=merged_data.na.drop()
merged_data_out.count

/* Dropping values from the data where absolute value of Handover Failure % is greater than 100% */
val merged_data=merged_data_out.where(col("handover_failure_pct")<=math.abs(100))
merged_data.count

/* Dropping values from the data where absolute value of adjusted_sip_sc_dc_perc is greater than 100 */
val merged_data=merged_data_out.where(col("adjusted_sip_sc_dc_pct")<=math.abs(100))
merged_data_out.count

/* Dropping values from the data where absolute value of adjusted_sip_sc_dc_perc is greater than 100 */
val merged_data_out=merged_data.where(col("adjusted_sip_sc_dc_pct")<=math.abs(100))
merged_data_out.count

/* Creating a column make_dispatch_decision Y(1) or N(0) depending on whether absolute value of SIP DC % is greater than 1% or not. */
val merged_data_fin=merged_data_out.withColumn("make_dispatch_decision",when(col("dc_perc")<=math.abs(1),1).otherwise(0))
-- This value of 1% can be adjusted.


XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.ml.feature.OneHotEncoder;
val freq_indexer = new StringIndexer().setInputCol("key_frequency").setOutputCol("freq_ind").fit(merged_data_out).transform(merged_data_out)
val freq_encoder = new OneHotEncoder().setInputCol("freq_ind").setOutputCol("freq_enc")
val merged_data_fin = freq_encoder.transform(freq_indexer) 
merged_data_fin.show(false)
XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

val merged_data_out=merged_data_fin

import org.apache.spark.ml.feature.StringIndexer
val indexer = new StringIndexer().setInputCol("key_frequency").setOutputCol("freq_enc").fit(merged_data_out)
val merged_data_fin = indexer.transform(merged_data_out)


import org.apache.spark.ml.feature.StringIndexer
val indexer = new StringIndexer().setInputCol("key_ramesh").setOutputCol("enode_enc").fit(merged_data_fin)
val merged_data_out = indexer.transform(merged_data_fin)

val merged_data_fin= merged_data_out

merged_data_fin.persist()

merged_data_fin.printSchema()

/* Creating a dataframe to store the mapping of actual radio number and the encoded radio number. */
-- This can be used in the future to predict values against the actual radio number as the model uses the encoded radio number. 
-- So, there has to be a way to retrieve the actual radio number and this method ensures that it is not lost.
val enode_num_df=merged_data_fin.select("key_ramesh","enode_enc").distinct

val freq_num_df=merged_data_fin.select("key_frequency","freq_enc").distinct


val data_actual_dc_perc=merged_data_fin.select("enode_enc","freq_enc","hr","cloudcover","precipintensity","temperature","uvindex", "windspeed", "pressure", "humidity","centerline_type", "make_dispatch_decision")

data_actual_dc_perc.printSchema()



/* Creating LabeledPointRDD for applying models. */
--Mapping each row to entities of corresponding datatype to create a vector values.
--Feature Vector is a vector having all the columns excpet the last one - the dependent variable.
--Label is a vector which contains the last column in the data - the dependent variable.
--The LabeledPointRDD dc_perc_rdd is formed by mapping the dataframe data_actual_dc_perc to the schema (label,featureVector) after the above transformation.
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression._
val dc_perc_rdd=data_actual_dc_perc.rdd.map(row => {val values = Array(row.getAs[Double](0),row.getAs[Double](1),row.getAs[Int](2),row.getAs[Double](3),row.getAs[Double](4),row.getAs[Double](5),row.getAs[Int](6),row.getAs[Double](7),row.getAs[Double](8),row.getAs[Double](9),row.getAs[Int](10),row.getAs[Int](11))
    val featureVector = Vectors.dense(values.init)
    val label = values.last
LabeledPoint(label,featureVector)})

val Array(trainData, testData) = dc_perc_rdd.randomSplit(Array(0.7, 0.3),100)

import org.apache.spark.mllib.tree.RandomForest
import org.apache.spark.mllib.tree.model.RandomForestModel
import org.apache.spark.mllib.util.MLUtils
val numClasses = 2
val categoricalFeaturesInfo = Map[Int, Int]()
val numTrees = 8
val featureSubsetStrategy = "auto" 
val impurity = "gini"
val maxDepth = 4
val maxBins = 100
val model_rf_clf = RandomForest.trainClassifier(trainData,numClasses,categoricalFeaturesInfo,numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)

val labelAndPreds = testData.map { point =>
  val prediction = model_rf_clf.predict(point.features)
  (point.label, prediction)
}

--Combined set of all the features,the label and the predicted value.
val featuresLabelsAndPredictions = testData.map { point =>
  val prediction = model_rf_clf.predict(point.features)
  (point.features,point.label, prediction)
}

--Sample records in featuresLabelsAndPredictions
featuresLabelsAndPredictions.take(10)

--Converting featuresLabelsAndPredictions to a dataframe.
import sqlContext.implicits._
val df_test_dc_perc=featuresLabelsAndPredictions.toDF()

--An UDF is created to split a vector into individual fields.
import org.apache.spark.sql.functions.udf
def columnExtractor(idx: Int) = udf((v: Vector) => v(idx))

--Applying the UDF on df_test_dc_perc. Now,df_test_dc_perc_pred contains the predicted data splitted correctly into columns.
val df_test_dc_perc_pred=df_test_dc_perc.withColumn("enode_enc",columnExtractor(0)($"_1")).withColumn("freq_enc",columnExtractor(1)($"_1")).withColumn("hr",columnExtractor(2)($"_1")).withColumn("cloudcover",columnExtractor(3)($"_1")).withColumn("precipintensity",columnExtractor(4)($"_1")).withColumn("temperature",columnExtractor(5)($"_1")).withColumn("uvindex",columnExtractor(6)($"_1")).withColumn("windspeed",columnExtractor(7)($"_1")).withColumn("pressure",columnExtractor(8)($"_1")).withColumn("humidity",columnExtractor(9)($"_1")).withColumn("centerline_type",columnExtractor(10)($"_1")).withColumn("make_dispatch_decision",$"_2").withColumn("predicted_decision",$"_3").select("enode_enc","freq_enc","hr","cloudcover","precipintensity","temperature","uvindex","windspeed","pressure","humidity","centerline_type","make_dispatch_decision","predicted_decision")

df_test_dc_perc_pred.show(10)

-- Joining with the radionumber dataframe previously created to get the actual value of the radio number.
val df_test_dc_perc_pred_fin=df_test_dc_perc_pred.join(enode_num_df,Seq("enode_enc"),"inner")

-- Joining with the radionumber dataframe previously created to get the actual value of the radio number.
val df_test_dc_perc_pred=df_test_dc_perc_pred_fin.join(freq_num_df,Seq("freq_enc"),"inner")

val df_test_dc_perc_pred_fin=df_test_dc_perc_pred


--Saving the entire actual and predicted test  dataset into a Hive table for future analysis.
df_test_dc_perc_pred_fin.write.mode("overwrite").format("parquet").saveAsTable("vz_classification_outlier")


val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()
scala> val testErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()
testErr: Double = 0.09971736833753397

println("Learned classification forest model:\n" + model_rf_clf.toDebugString)

/* Metrics */
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics

--Instantiate metrics object
val metrics_dispatch = new BinaryClassificationMetrics(labelAndPreds)

--Precision
val precision = metrics_dispatch.precisionByThreshold
println(precision)
precision: Double = 0.900282631662466
--Recall
val recall = metrics_dispatch.recall 
recall: Double = 0.900282631662466
--F1-Score
val f1Score = metrics_dispatch.fMeasure
f1Score: Double = 0.900282631662466

